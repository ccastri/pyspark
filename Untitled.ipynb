{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c097a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\camil\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\camil\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\camil\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70d5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc24c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ee7d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of         name  age  risk\n",
      "0      Pilar   53     1\n",
      "1  Francisco   60     2\n",
      "2  Sebastian   28     3\n",
      "3     Camilo   25     4\n",
      "4     Camilo   25     5>\n",
      "             age      risk\n",
      "count   5.000000  5.000000\n",
      "mean   38.200000  3.000000\n",
      "std    16.932218  1.581139\n",
      "min    25.000000  1.000000\n",
      "25%    25.000000  2.000000\n",
      "50%    28.000000  3.000000\n",
      "75%    53.000000  4.000000\n",
      "max    60.000000  5.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"test1.csv\")\n",
    "print(df.info)\n",
    "print(df.describe()) #This is a cool one dude!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7787d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('first_app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad528164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Castri:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>first_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2250ad8ab20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad4c6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6fc1a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, _c1: string, _c2: string]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    5 non-null      object\n",
      " 1   age     5 non-null      int64 \n",
      " 2   risk    5 non-null      int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 248.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "print(df_pyspark)\n",
    "# type(df_pyspark)\n",
    "df.info() # pandas version of schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9527241f",
   "metadata": {},
   "source": [
    "print(spark.read.option('header', 'true').csv('test1.csv')) #to make 1st raw the headers\n",
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv') #look at the dif types\n",
    "print(type(df_pyspark))\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ed201",
   "metadata": {},
   "source": [
    "## 2nd dive into pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d766ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string, age: string, risk: string]\n",
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to have the correct columntype \n",
    "df_2 = spark.read.option('header', 'true').csv('test1.csv', inferSchema = True) \n",
    "print(spark.read.option('header', 'true').csv('test1.csv')) #check types in schema\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa4fe34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- risk: integer (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    " #Checking the types and the Schema\n",
    "print(df_2.printSchema())\n",
    "print(type(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0fcb6669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Pilar', age=53, risk=1),\n",
       " Row(name='Francisco', age=60, risk=2),\n",
       " Row(name='Sebastian', age=28, risk=3)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0011289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "488f9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     name|risk|\n",
      "+---------+----+\n",
      "|    Pilar|   1|\n",
      "|Francisco|   2|\n",
      "|Sebastian|   3|\n",
      "|   Camilo|   4|\n",
      "|   Camilo|   5|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type(df_2.select('name')) #to check the data type on the column (pyspark.sql.dataframe.DataFrame)\n",
    "df_2.select(['name', 'risk']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b4b8b5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With pandas it'll go like this:\n",
    "df_2['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8b12a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('risk', 'int')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9f199742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------------+------------------+\n",
      "|summary|     name|              age|              risk|\n",
      "+-------+---------+-----------------+------------------+\n",
      "|  count|        5|                5|                 5|\n",
      "|   mean|     null|             38.2|               3.0|\n",
      "| stddev|     null|16.93221781102523|1.5811388300841898|\n",
      "|    min|   Camilo|               25|                 1|\n",
      "|    max|Sebastian|               60|                 5|\n",
      "+-------+---------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7ae3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string, age: int, risk: int, age after two years: int]\n",
      "+---------+---+----+-------------------+\n",
      "|     name|age|risk|age after two years|\n",
      "+---------+---+----+-------------------+\n",
      "|    Pilar| 53|   1|                 55|\n",
      "|Francisco| 60|   2|                 62|\n",
      "|Sebastian| 28|   3|                 30|\n",
      "|   Camilo| 25|   4|                 27|\n",
      "|   Camilo| 25|   5|                 27|\n",
      "+---------+---+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_2.withColumn('age after two years', df_2['age']+2))#NOT INPLACE must be assigned\n",
    "df_2 = df_2.withColumn('age after two years', df_2['age']+2)\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a51553a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = df_2.drop('age after two years')\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10f02464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     Name|Age|Risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Rename the columns\n",
    "df_2.withColumnRenamed('name', 'Name',).withColumnRenamed(\"age\", \"Age\").withColumnRenamed(\"risk\", \"Risk\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d54f4",
   "metadata": {},
   "source": [
    "# How to handle missing values in pyspark\n",
    "### -Dropping cols\n",
    "### -Dropping rows\n",
    "### -Handling missing values and statistic parameters (mean, mode and median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056ba47",
   "metadata": {},
   "source": [
    "### Avoiding a imputer function to clean up the missing values (Data inputation from pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7e0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+\n",
      "|     name| age|risk|\n",
      "+---------+----+----+\n",
      "|    Pilar|  53|   1|\n",
      "|Francisco|  60|   2|\n",
      "|Sebastian|  28|   3|\n",
      "|   Camilo|  25|   4|\n",
      "|   Camilo|  25|   5|\n",
      "|   Daniel|   3|null|\n",
      "|Valentina|null|   3|\n",
      "|    David|null|null|\n",
      "+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = spark.read.csv('test2.csv', header= True, inferSchema= True)\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "198b241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# df_3.na.drop(how=\"any\", thresh=None, subset=None) \n",
    "any, all\n",
    "1,2,3...,n, etc. (the number of null values)\n",
    "subset= for checking specific columns ([\"age\"], [\"name\",\"age\"])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_3.na.drop().show() # To drop all rows with at least 1 null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "649a76db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "|     null|  3|   3|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.na.drop(how='any', thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "89963c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+\n",
      "|     name|age|risk|\n",
      "+---------+---+----+\n",
      "|    Pilar| 53|   1|\n",
      "|Francisco| 60|   2|\n",
      "|Sebastian| 28|   3|\n",
      "|   Camilo| 25|   4|\n",
      "|   Camilo| 25|   5|\n",
      "+---------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.na.drop(how='any', subset=[\"name\",\"risk\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "09e53f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----+\n",
      "|          name|age|risk|\n",
      "+--------------+---+----+\n",
      "|         Pilar| 53|   1|\n",
      "|     Francisco| 60|   2|\n",
      "|     Sebastian| 28|   3|\n",
      "|        Camilo| 25|   4|\n",
      "|        Camilo| 25|   5|\n",
      "|Missing Values|  3|null|\n",
      "|Missing Values|  3|   3|\n",
      "+--------------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por si acaso necesito llenar una fila de null de tipo entero :P\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "df_3 = df_3.withColumn(\"risk\", df_3[\"risk\"].cast(StringType()))\n",
    "df_3_filled = df_3.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618c2ac",
   "metadata": {},
   "source": [
    "# Pyspark inputer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d53959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "inputCols= ['age', 'risk'],\n",
    "outputCols=[\"{}_imputed\".format(c) for c in  [ 'age', 'risk']]\n",
    ").setStrategy(\"mode\")\n",
    "# mean mode can be used too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3ad099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+-----------+------------+\n",
      "|     name| age|risk|age_imputed|risk_imputed|\n",
      "+---------+----+----+-----------+------------+\n",
      "|    Pilar|  53|   1|         53|           1|\n",
      "|Francisco|  60|   2|         60|           2|\n",
      "|Sebastian|  28|   3|         28|           3|\n",
      "|   Camilo|  25|   4|         25|           4|\n",
      "|   Camilo|  25|   5|         25|           5|\n",
      "|   Daniel|   3|null|          3|           3|\n",
      "|Valentina|null|   3|         25|           3|\n",
      "|    David|null|null|         25|           3|\n",
      "+---------+----+----+-----------+------------+\n",
      "\n",
      "[('name', 'string'), ('age', 'int'), ('risk', 'int')]\n"
     ]
    }
   ],
   "source": [
    "#Add imputation cols to df\n",
    "\n",
    "imputer.fit(df_3).transform(df_3).show()\n",
    "print(df_3.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75652052",
   "metadata": {},
   "source": [
    "# Pyspark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cdb12",
   "metadata": {},
   "source": [
    "## Filter dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ca7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+------+\n",
      "|     name|age|risk|Salary|\n",
      "+---------+---+----+------+\n",
      "|    Pilar| 53|   1|530000|\n",
      "|Francisco| 60|   2|600000|\n",
      "|Sebastian| 28|   3|280000|\n",
      "|   Camilo| 25|   4|250000|\n",
      "|   Camilo| 25|   5|250000|\n",
      "+---------+---+----+------+\n",
      "\n",
      "+------+---+----+------+\n",
      "|  name|age|risk|Salary|\n",
      "+------+---+----+------+\n",
      "|Camilo| 25|   4|250000|\n",
      "|Camilo| 25|   5|250000|\n",
      "+------+---+----+------+\n",
      "\n",
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Camilo| 25|\n",
      "|Camilo| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Salary for people lesser 20000\n",
    "df_4= spark.read.csv(\"test1.csv\", header = True, inferSchema= True)\n",
    "df_4 = df_4.withColumn('Salary', df_4['age']*10000)\n",
    "df_4.show()\n",
    "df_4.filter('salary <= 260000' ).show()\n",
    "df_4.filter('salary <= 260000' ).select(['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c76fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+------+\n",
      "|     name|age|risk|Salary|\n",
      "+---------+---+----+------+\n",
      "|Sebastian| 28|   3|280000|\n",
      "|   Camilo| 25|   4|250000|\n",
      "|   Camilo| 25|   5|250000|\n",
      "+---------+---+----+------+\n",
      "\n",
      "+---------+---+----+------+\n",
      "|     name|age|risk|Salary|\n",
      "+---------+---+----+------+\n",
      "|    Pilar| 53|   1|530000|\n",
      "|Francisco| 60|   2|600000|\n",
      "+---------+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_4.filter((df_4['Salary'] <= 300000) &\n",
    "           (df_4['Salary'] >= 250000)).show()\n",
    "# Inverse opperator (~)\n",
    "df_4.filter(~(df_4['Salary'] <= 300000) &\n",
    "           (df_4['Salary'] >= 250000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "279afa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, sum(age): bigint, sum(risk): bigint, sum(Salary): bigint]\n",
      "+---------+--------+---------+-----------+\n",
      "|     Name|sum(age)|sum(risk)|sum(Salary)|\n",
      "+---------+--------+---------+-----------+\n",
      "|Sebastian|      28|        3|     280000|\n",
      "|    Pilar|      53|        1|     530000|\n",
      "|   Camilo|      50|        9|     500000|\n",
      "|Francisco|      60|        2|     600000|\n",
      "+---------+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_4.groupBy('Name').sum())\n",
    "df_4.groupBy('Name').sum().show() #\"\"\" Got confused because bith age, risk and salary are all integers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bcdc6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+------+\n",
      "|     name|        position|Salary|\n",
      "+---------+----------------+------+\n",
      "|    Pilar|          Lawyer| 53000|\n",
      "|Francisco|          Lawyer| 60000|\n",
      "|Sebastian|      Management| 28000|\n",
      "|   Camilo|    Data science|250000|\n",
      "|   Camilo|        frontend| 25000|\n",
      "|   Daniel|       Marketing|  3000|\n",
      "|Valentina|        Cleaning| 13000|\n",
      "|    David|customer service| 10000|\n",
      "+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5 = spark.read.csv(\"test3.csv\", header=True, inferSchema=True)\n",
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c66dc327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     name|sum(Salary)|\n",
      "+---------+-----------+\n",
      "|Valentina|      13000|\n",
      "|Sebastian|      28000|\n",
      "|   Daniel|       3000|\n",
      "|    Pilar|      53000|\n",
      "|   Camilo|     275000|\n",
      "|    David|      10000|\n",
      "|Francisco|      60000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5.groupBy('name').sum().show() #Encontrar quien tiene el salario mas alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3cbd52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|        position|sum(Salary)|\n",
      "+----------------+-----------+\n",
      "|        frontend|      25000|\n",
      "|      Management|      28000|\n",
      "|          Lawyer|     113000|\n",
      "|    Data science|     250000|\n",
      "|       Marketing|       3000|\n",
      "|        Cleaning|      13000|\n",
      "|customer service|      10000|\n",
      "+----------------+-----------+\n",
      "\n",
      "+----------------+-----------+\n",
      "|        position|avg(Salary)|\n",
      "+----------------+-----------+\n",
      "|        frontend|    25000.0|\n",
      "|      Management|    28000.0|\n",
      "|          Lawyer|    56500.0|\n",
      "|    Data science|   250000.0|\n",
      "|       Marketing|     3000.0|\n",
      "|        Cleaning|    13000.0|\n",
      "|customer service|    10000.0|\n",
      "+----------------+-----------+\n",
      "\n",
      "+----------------+-----+\n",
      "|        position|count|\n",
      "+----------------+-----+\n",
      "|        frontend|    1|\n",
      "|      Management|    1|\n",
      "|          Lawyer|    2|\n",
      "|    Data science|    1|\n",
      "|       Marketing|    1|\n",
      "|        Cleaning|    1|\n",
      "|customer service|    1|\n",
      "+----------------+-----+\n",
      "\n",
      "+----------------+-----------+\n",
      "|        position|max(Salary)|\n",
      "+----------------+-----------+\n",
      "|        frontend|      25000|\n",
      "|      Management|      28000|\n",
      "|          Lawyer|      60000|\n",
      "|    Data science|     250000|\n",
      "|       Marketing|       3000|\n",
      "|        Cleaning|      13000|\n",
      "|customer service|      10000|\n",
      "+----------------+-----------+\n",
      "\n",
      "+----------------+-----------+\n",
      "|        position|min(Salary)|\n",
      "+----------------+-----------+\n",
      "|        frontend|      25000|\n",
      "|      Management|      28000|\n",
      "|          Lawyer|      53000|\n",
      "|    Data science|     250000|\n",
      "|       Marketing|       3000|\n",
      "|        Cleaning|      13000|\n",
      "|customer service|      10000|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5.groupBy('position').sum().show() \n",
    "df_5.groupBy('position').mean().show()\n",
    "df_5.groupBy('position').count().show()\n",
    "df_5.groupBy('position').max().show() \n",
    "df_5.groupBy('position').min().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb967626",
   "metadata": {},
   "source": [
    "# ML intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c061581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+\n",
      "|     name|age|Salary|experience|\n",
      "+---------+---+------+----------+\n",
      "|    Pilar| 53| 53000|        10|\n",
      "|Francisco| 60| 60000|         3|\n",
      "|Sebastian| 28| 28000|        20|\n",
      "|   Camilo| 25|250000|        30|\n",
      "|   Camilo| 13| 25000|         4|\n",
      "|   Daniel| 34|  3000|        20|\n",
      "|Valentina| 50| 13000|        50|\n",
      "|    David| 39| 10000|         2|\n",
      "+---------+---+------+----------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'Salary', 'experience']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = spark.read.csv(\"test4.csv\", header=True, inferSchema=True)\n",
    "training.show()\n",
    "training.printSchema()\n",
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbba97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_assembler = VectorAssembler(inputCols=[\"age\", \"experience\"], outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3838d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+----------+--------------------+\n",
      "|     name|age|Salary|experience|Independent Features|\n",
      "+---------+---+------+----------+--------------------+\n",
      "|    Pilar| 53| 53000|        10|         [53.0,10.0]|\n",
      "|Francisco| 60| 60000|         3|          [60.0,3.0]|\n",
      "|Sebastian| 28| 28000|        20|         [28.0,20.0]|\n",
      "|   Camilo| 25|250000|        30|         [25.0,30.0]|\n",
      "|   Camilo| 13| 25000|         4|          [13.0,4.0]|\n",
      "|   Daniel| 34|  3000|        20|         [34.0,20.0]|\n",
      "|Valentina| 50| 13000|        50|         [50.0,50.0]|\n",
      "|    David| 39| 10000|         2|          [39.0,2.0]|\n",
      "+---------+---+------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = feature_assembler.transform(training)\n",
    "output.show()\n",
    "## Output shows age and experience as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2503b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'Salary', 'experience', 'Independent Features']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ac16993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [53.0,10.0]| 53000|\n",
      "|          [60.0,3.0]| 60000|\n",
      "|         [28.0,20.0]| 28000|\n",
      "|         [25.0,30.0]|250000|\n",
      "|          [13.0,4.0]| 25000|\n",
      "|         [34.0,20.0]|  3000|\n",
      "|         [50.0,50.0]| 13000|\n",
      "|          [39.0,2.0]| 10000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data = output.select(\"Independent Features\", \"Salary\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab7fa470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Set up train test split\n",
    "train_data, test_data= finalized_data.randomSplit([0.75, 0.25])\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol = 'Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0e73cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([375.3798, 6096.5242])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b45714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-23688.395663072584"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85be5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|         [28.0,20.0]| 28000|117264.14360873653|\n",
      "|         [50.0,50.0]| 13000|308418.22498797753|\n",
      "|         [53.0,10.0]| 53000| 65683.39671123934|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Prediction\n",
    "pred_result = regressor.evaluate(test_data)\n",
    "pred_result.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e6f96ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpred_results\u001b[49m\u001b[38;5;241m.\u001b[39mmeanAbsoluteError,pred_results\u001b[38;5;241m.\u001b[39mmeanSquaredError\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_results' is not defined"
     ]
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_result.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09114966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
